{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0cca7178",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baseline Trained. Global Mean: 7.82\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy.sparse import csr_matrix\n",
    "from sklearn.metrics import ndcg_score, mean_squared_error\n",
    "\n",
    "# 1. Load Data (Same Split)\n",
    "train_df = pd.read_csv('train.csv')\n",
    "test_df = pd.read_csv('test.csv')\n",
    "\n",
    "# 2. Compute \"The Model\" (Just a dictionary of averages)\n",
    "# User Mean: \"How generous is this user?\"\n",
    "user_means = train_df.groupby('User_ID')['Book_Rating'].mean().to_dict()\n",
    "\n",
    "# Global Mean: \"What is the average rating generally?\" (Fallback)\n",
    "global_mean = train_df['Book_Rating'].mean()\n",
    "\n",
    "# Popularity List (Needed for ranking because User Mean gives ties)\n",
    "# We calculate the \"Top Books\" to recommend when we can't distinguish items\n",
    "popular_books = train_df.groupby('Book_Title')['Book_Rating'].count() \\\n",
    "    .sort_values(ascending=False).index.tolist()\n",
    "\n",
    "# 3. Create Helpers for Scorecard\n",
    "# (Same setup as NCF/Cosine to ensure fair comparison)\n",
    "df_full = pd.concat([train_df, test_df])\n",
    "\n",
    "# We need encoders just to build the matrix for the Referee\n",
    "# (The baseline model doesn't use them, but the Scorecard needs them)\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "user_enc = LabelEncoder()\n",
    "df_full['user_encoded'] = user_enc.fit_transform(df_full['User_ID'])\n",
    "book_enc = LabelEncoder()\n",
    "df_full['book_encoded'] = book_enc.fit_transform(df_full['Book_Title'])\n",
    "\n",
    "# Split back to get the Test Matrix\n",
    "test_df_encoded = df_full.iloc[len(train_df):]\n",
    "n_users = df_full['user_encoded'].max() + 1\n",
    "n_books = df_full['book_encoded'].max() + 1\n",
    "\n",
    "test_m = csr_matrix(\n",
    "    (test_df_encoded['Book_Rating'].values, \n",
    "     (test_df_encoded['user_encoded'].values, test_df_encoded['book_encoded'].values)), \n",
    "    shape=(n_users, n_books)\n",
    ")\n",
    "\n",
    "# Popularity Dict for Novelty Score\n",
    "book_pop_dict = df_full.groupby('book_encoded')['Book_Rating'].count().to_dict()\n",
    "\n",
    "print(f\"Baseline Trained. Global Mean: {global_mean:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ed65272e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted Score: 7.8193204757156405\n"
     ]
    }
   ],
   "source": [
    "def predict_rating_user_avg(user_id, book_title):\n",
    "    # 1. Check if we know this user\n",
    "    if user_id in user_means:\n",
    "        return user_means[user_id]\n",
    "    \n",
    "    # 2. Cold Start: Return Global Average\n",
    "    return global_mean\n",
    "\n",
    "# Test\n",
    "# If User 276747 usually gives 8s, this should print 8.0\n",
    "print(f\"Predicted Score: {predict_rating_user_avg(276747, 'Any Book')}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9d7c5c91",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Baseline Recommendations for User 276747 ---\n",
      "7.82 | The Lovely Bones: A Novel\n",
      "7.82 | The Da Vinci Code\n",
      "7.82 | The Secret Life of Bees\n",
      "7.82 | Bridget Jones's Diary\n",
      "7.82 | Wild Animus\n"
     ]
    }
   ],
   "source": [
    "def recommend_user_avg(user_id, n_recommendations=5):\n",
    "    # 1. Get the score we will predict for EVERYTHING\n",
    "    predicted_score = user_means.get(user_id, global_mean)\n",
    "    \n",
    "    # 2. Get Top Popular Books (that user hasn't read)\n",
    "    # (Simplified: Just taking top N popular for the baseline demo)\n",
    "    recs = popular_books[:n_recommendations]\n",
    "    \n",
    "    print(f\"--- Baseline Recommendations for User {user_id} ---\")\n",
    "    results = []\n",
    "    for title in recs:\n",
    "        # The score is constant!\n",
    "        print(f\"{predicted_score:.2f} | {title}\")\n",
    "        results.append(title)\n",
    "        \n",
    "    return results\n",
    "\n",
    "# Test\n",
    "recs = recommend_user_avg(276747)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e782220e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Scoring Model: User Average Baseline ---\n",
      "{'Model': 'User Average Baseline', 'RMSE': np.float64(1.5308406216825525), 'NDCG': np.float64(0.9618240823162386), 'Novelty': np.float64(55.28770287141073)}\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import ndcg_score, mean_squared_error\n",
    "\n",
    "# 1. Create Helper Mapping (Required for the wrapper)\n",
    "# Maps the Integers (0, 1, 2...) back to Original User IDs (276747...)\n",
    "int_to_user_id = dict(zip(df_full['user_encoded'], df_full['User_ID']))\n",
    "\n",
    "# 2. Define the Wrapper Function\n",
    "def predict_baseline_wrapper(u_enc, b_enc):\n",
    "    # Convert Encoded Integer -> Original User ID\n",
    "    user_id = int_to_user_id.get(u_enc)\n",
    "    \n",
    "    # Predict (Constant score for this user)\n",
    "    if user_id in user_means:\n",
    "        return user_means[user_id]\n",
    "    return global_mean\n",
    "\n",
    "# 3. Define the Scorecard Function (With Novelty included)\n",
    "def get_model_scorecard(model_name, test_data_matrix, prediction_function, book_popularity_dict):\n",
    "    print(f\"--- Scoring Model: {model_name} ---\")\n",
    "    rmses = []\n",
    "    ndcg_scores = []\n",
    "    novelty_scores = []\n",
    "    \n",
    "    test_users = np.unique(test_data_matrix.nonzero()[0])\n",
    "    sample_users = np.random.choice(test_users, size=min(500, len(test_users)), replace=False)\n",
    "    \n",
    "    for u in sample_users:\n",
    "        true_ratings = test_data_matrix[u].data\n",
    "        if len(true_ratings) < 2: continue \n",
    "        \n",
    "        # Predict\n",
    "        true_book_ids = test_data_matrix[u].indices\n",
    "        pred_ratings = [prediction_function(u, b) for b in true_book_ids]\n",
    "            \n",
    "        # Metrics\n",
    "        rmses.append(np.sqrt(mean_squared_error(true_ratings, pred_ratings)))\n",
    "        try:\n",
    "            ndcg_scores.append(ndcg_score([true_ratings], [pred_ratings]))\n",
    "        except: pass\n",
    "        \n",
    "        # Novelty\n",
    "        top_k_idx = np.argsort(pred_ratings)[::-1][:5]\n",
    "        top_books = true_book_ids[top_k_idx]\n",
    "        pop_score = np.mean([book_popularity_dict.get(b, 0) for b in top_books])\n",
    "        novelty_scores.append(pop_score)\n",
    "\n",
    "    return {\n",
    "        \"Model\": model_name,\n",
    "        \"RMSE\": np.mean(rmses),\n",
    "        \"NDCG\": np.mean(ndcg_scores),\n",
    "        \"Novelty\": np.mean(novelty_scores)\n",
    "    }\n",
    "\n",
    "# 4. Run Evaluation\n",
    "baseline_scores = get_model_scorecard(\"User Average Baseline\", test_m, predict_baseline_wrapper, book_pop_dict)\n",
    "print(baseline_scores)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
